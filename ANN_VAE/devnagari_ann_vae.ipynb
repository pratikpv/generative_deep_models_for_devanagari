{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n",
      "Using GPU: NVIDIA TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print('Running on device:', device)\n",
    "if use_cuda:\n",
    "    print('Using GPU:',\n",
    "          torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/therock/data2/devnagari_data/'\n",
    "\n",
    "expr_name = 'devnagari_ann_vae'\n",
    "model_name = expr_name + '_PyTorch_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "# each image in dataset is 32x32 pixels\n",
    "image_dim = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize(image_dim),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "train_data = datasets.ImageFolder(os.path.join(root, 'Train'),\n",
    "                                  transform=train_transform)\n",
    "\n",
    "train_data_len = len(train_data)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_out_dir = expr_name + '_decoded'\n",
    "if not os.path.exists(decoded_out_dir):\n",
    "    os.mkdir(decoded_out_dir)\n",
    "\n",
    "\n",
    "def to_img(x):\n",
    "    x = x.view(x.size(0), 1, 32, 32)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEEncoder(nn.Module):\n",
    "    def __init__(self, in_dim=10):\n",
    "\n",
    "        super(VAEEncoder, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.fc1_dim = int(in_dim * 0.80)\n",
    "        self.fc2_dim = int(in_dim * 0.50)\n",
    "        self.latent_dim = int(in_dim * 0.30)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.in_dim, self.fc1_dim)\n",
    "        self.fc2 = nn.Linear(self.fc1_dim, self.fc2_dim)\n",
    "        self.mu = nn.Linear(self.fc2_dim, self.latent_dim)\n",
    "        self.var = nn.Linear(self.fc2_dim, self.latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        latent_mu = self.mu(x)\n",
    "        latent_var = self.var(x)\n",
    "        return latent_mu, latent_var\n",
    "\n",
    "\n",
    "class VAEDecoder(nn.Module):\n",
    "    def __init__(self, in_dim=10):\n",
    "\n",
    "        super(VAEDecoder, self).__init__()\n",
    "        self.latent_dim = int(in_dim * 0.30)\n",
    "        self.fc1_dim = int(in_dim * 0.50)\n",
    "        self.fc2_dim = int(in_dim * 0.80)\n",
    "        self.out_dim = in_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(self.latent_dim, self.fc1_dim)\n",
    "        self.fc2 = nn.Linear(self.fc1_dim, self.fc2_dim)\n",
    "        self.out = nn.Linear(self.fc2_dim, self.out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        pred = torch.sigmoid(self.out(x))\n",
    "        return pred\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, encd, decd):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = encd\n",
    "        self.decoder = decd\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_mu, latent_var = self.encoder(x)\n",
    "\n",
    "        std = torch.exp(latent_var / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        x_sample = eps.mul(std).add_(latent_mu)\n",
    "\n",
    "        # decode\n",
    "        predicted = self.decoder(x_sample)\n",
    "        return predicted, latent_mu, latent_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (encoder): VAEEncoder(\n",
      "    (fc1): Linear(in_features=1024, out_features=819, bias=True)\n",
      "    (fc2): Linear(in_features=819, out_features=512, bias=True)\n",
      "    (mu): Linear(in_features=512, out_features=307, bias=True)\n",
      "    (var): Linear(in_features=512, out_features=307, bias=True)\n",
      "  )\n",
      "  (decoder): VAEDecoder(\n",
      "    (fc1): Linear(in_features=307, out_features=512, bias=True)\n",
      "    (fc2): Linear(in_features=512, out_features=819, bias=True)\n",
      "    (out): Linear(in_features=819, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "in_dim = image_dim * image_dim\n",
    "\n",
    "encoder = VAEEncoder(in_dim=in_dim)\n",
    "decoder = VAEDecoder(in_dim=in_dim)\n",
    "\n",
    "model = VAE(encoder, decoder)\n",
    "if use_cuda:\n",
    "    model = model.to(device)\n",
    "print(model)\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "lowest_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 : 838656\n",
      " 1 : 819\n",
      " 2 : 419328\n",
      " 3 : 512\n",
      " 4 : 157184\n",
      " 5 : 307\n",
      " 6 : 157184\n",
      " 7 : 307\n",
      " 8 : 157184\n",
      " 9 : 512\n",
      "10 : 419328\n",
      "11 : 819\n",
      "12 : 838656\n",
      "13 : 1024\n",
      "==========\n",
      "2991820\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for i, item in enumerate(params):\n",
    "        print(f'{i:2} : {item:}')\n",
    "    print(f'==========\\n{sum(params):>6}')\n",
    "\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(image_dim, train_loader, model, e):\n",
    "    global lowest_loss\n",
    "    # set the train mode\n",
    "    model.train()\n",
    "\n",
    "    # loss of the epoch\n",
    "    train_loss = 0\n",
    "\n",
    "    for i, (x, _) in enumerate(train_loader):\n",
    "        # reshape the data into [batch_size, image_dim * image_dim]\n",
    "        x = x.view(-1, image_dim * image_dim)\n",
    "        x = x.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        x_sample, latent_mu, latent_var = model(x)\n",
    "\n",
    "        if e % 10 == 0:\n",
    "            pic = to_img(x_sample.cpu().data)\n",
    "            save_image(pic, './{}/image_{}.png'.format(decoded_out_dir, e))\n",
    "\n",
    "        # reconstruction loss\n",
    "        recon_loss = F.binary_cross_entropy(x_sample, x, reduction='sum')\n",
    "        # kl divergence loss\n",
    "        kl_loss = 0.5 * torch.sum(\n",
    "            torch.exp(latent_var) + latent_mu**2 - 1.0 - latent_var)\n",
    "\n",
    "        # total loss\n",
    "        loss = recon_loss + kl_loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if loss.item() < lowest_loss:\n",
    "            lowest_loss = loss.item()\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "            #print(f\"saved model\")\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 441.86043563179345\n",
      "Epoch 1, Train Loss: 385.55062005474747\n",
      "Epoch 2, Train Loss: 339.82001113930625\n",
      "Epoch 3, Train Loss: 307.2561562899616\n",
      "Epoch 4, Train Loss: 287.12344104659525\n",
      "Epoch 5, Train Loss: 274.1087774736253\n",
      "Epoch 6, Train Loss: 265.43618648597345\n",
      "Epoch 7, Train Loss: 259.62570959378996\n",
      "Epoch 8, Train Loss: 255.52938831222028\n",
      "Epoch 9, Train Loss: 252.05234045316496\n",
      "Epoch 10, Train Loss: 248.64785965473146\n",
      "Epoch 11, Train Loss: 246.40685641783887\n",
      "Epoch 12, Train Loss: 244.47322355538682\n",
      "Epoch 13, Train Loss: 242.96627729879316\n",
      "Epoch 14, Train Loss: 241.57690102501599\n",
      "Epoch 15, Train Loss: 240.3479739450128\n",
      "Epoch 16, Train Loss: 239.16503946211637\n",
      "Epoch 17, Train Loss: 237.9136608106218\n",
      "Epoch 18, Train Loss: 237.05326389166402\n",
      "Epoch 19, Train Loss: 236.24863298733217\n",
      "Epoch 20, Train Loss: 235.65201179367807\n",
      "Epoch 21, Train Loss: 234.93606530230977\n",
      "Epoch 22, Train Loss: 234.15479115049553\n",
      "Epoch 23, Train Loss: 233.3901244804987\n",
      "Epoch 24, Train Loss: 232.64243271459398\n",
      "Epoch 25, Train Loss: 231.93082635669757\n",
      "Epoch 26, Train Loss: 231.3291706961317\n",
      "Epoch 27, Train Loss: 230.72785713215313\n",
      "Epoch 28, Train Loss: 230.22165046555307\n",
      "Epoch 29, Train Loss: 229.68641052089993\n",
      "Epoch 30, Train Loss: 229.03103895260548\n",
      "Epoch 31, Train Loss: 228.6114776564498\n",
      "Epoch 32, Train Loss: 228.32312919597186\n",
      "Epoch 33, Train Loss: 228.04606267982737\n",
      "Epoch 34, Train Loss: 227.76157676130913\n",
      "Epoch 35, Train Loss: 227.35129668018703\n",
      "Epoch 36, Train Loss: 227.03933358975382\n",
      "Epoch 37, Train Loss: 226.9159891504156\n",
      "Epoch 38, Train Loss: 226.64314990109494\n",
      "Epoch 39, Train Loss: 226.2394501778293\n",
      "Epoch 40, Train Loss: 226.1202366727941\n",
      "Epoch 41, Train Loss: 225.85585815117489\n",
      "Epoch 42, Train Loss: 225.72391149496482\n",
      "Epoch 43, Train Loss: 225.4579764426151\n",
      "Epoch 44, Train Loss: 225.23212281210039\n",
      "Epoch 45, Train Loss: 224.98562450047953\n",
      "Epoch 46, Train Loss: 224.59802070012788\n",
      "Epoch 47, Train Loss: 224.42220211097347\n",
      "Epoch 48, Train Loss: 224.22826626438618\n",
      "Epoch 49, Train Loss: 224.1568644601183\n",
      "Epoch 50, Train Loss: 223.82712678328804\n",
      "Epoch 51, Train Loss: 223.61999500479538\n",
      "Epoch 52, Train Loss: 223.48639933164162\n",
      "Epoch 53, Train Loss: 223.32449091372283\n",
      "Epoch 54, Train Loss: 223.1429179737452\n",
      "Epoch 55, Train Loss: 223.11713250279732\n",
      "Epoch 56, Train Loss: 223.06223797654252\n",
      "Epoch 57, Train Loss: 222.7833959398977\n",
      "Epoch 58, Train Loss: 222.77239285286126\n",
      "Epoch 59, Train Loss: 222.68191998181746\n",
      "Epoch 60, Train Loss: 222.49386454004156\n",
      "Epoch 61, Train Loss: 222.23122082800512\n",
      "Epoch 62, Train Loss: 222.15148874580402\n",
      "Epoch 63, Train Loss: 222.00125761768703\n",
      "Epoch 64, Train Loss: 222.0403870534287\n",
      "Epoch 65, Train Loss: 221.83867552149937\n",
      "Epoch 66, Train Loss: 221.73661724744247\n",
      "Epoch 67, Train Loss: 221.63994350423593\n",
      "Epoch 68, Train Loss: 221.67958130195012\n",
      "Epoch 69, Train Loss: 221.45253336796677\n",
      "Epoch 70, Train Loss: 221.40945162643862\n",
      "Epoch 71, Train Loss: 221.2698462476023\n",
      "Epoch 72, Train Loss: 221.12529674012947\n",
      "Epoch 73, Train Loss: 221.10609789601983\n",
      "Epoch 74, Train Loss: 220.97549777213874\n",
      "Epoch 75, Train Loss: 220.77831928848306\n",
      "Epoch 76, Train Loss: 220.88095565756873\n",
      "Epoch 77, Train Loss: 220.785420995844\n",
      "Epoch 78, Train Loss: 220.69586384570812\n",
      "Epoch 79, Train Loss: 220.64480628596547\n",
      "Epoch 80, Train Loss: 220.54256893382353\n",
      "Epoch 81, Train Loss: 220.3104582101183\n",
      "Epoch 82, Train Loss: 220.4071787084399\n",
      "Epoch 83, Train Loss: 220.25499745244565\n",
      "Epoch 84, Train Loss: 220.189377147938\n",
      "Epoch 85, Train Loss: 220.07465098505435\n",
      "Epoch 86, Train Loss: 219.97550506513747\n",
      "Epoch 87, Train Loss: 219.97645385429988\n",
      "Epoch 88, Train Loss: 219.93810139765824\n",
      "Epoch 89, Train Loss: 219.89498346587277\n",
      "Epoch 90, Train Loss: 219.70626133911446\n",
      "Epoch 91, Train Loss: 219.62714024536444\n",
      "Epoch 92, Train Loss: 219.5719631154092\n",
      "Epoch 93, Train Loss: 219.47096182664643\n",
      "Epoch 94, Train Loss: 219.5930769461317\n",
      "Epoch 95, Train Loss: 219.46221457400895\n",
      "Epoch 96, Train Loss: 219.28853300831202\n",
      "Epoch 97, Train Loss: 219.25330842391304\n",
      "Epoch 98, Train Loss: 219.11114130434783\n",
      "Epoch 99, Train Loss: 219.1568054417759\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for e in range(num_epochs):\n",
    "\n",
    "    train_loss = train(image_dim, train_loader, model, e)\n",
    "    train_loss /= len(train_data)\n",
    "    print(f'Epoch {e}, Train Loss: {train_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = image_dim * image_dim\n",
    "latent_dim = int(in_dim * 0.30)\n",
    "\n",
    "encoder_test = VAEEncoder(in_dim=in_dim)\n",
    "decoder_test = VAEDecoder(in_dim=in_dim)\n",
    "\n",
    "model_test = VAE(encoder_test, decoder_test)\n",
    "model_test.load_state_dict = torch.load(model_name)\n",
    "model.eval()\n",
    "\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "    \n",
    "sample_batches = 20\n",
    "decoded_data = torch.FloatTensor(batch_size,1,image_dim,image_dim)\n",
    "\n",
    "for sb_ in range(sample_batches):\n",
    "    for i in range(batch_size):\n",
    "        z = torch.randn(1, latent_dim).to(device)\n",
    "        reconstructed_img = model.decoder(z).to('cpu')\n",
    "        img = reconstructed_img.view(image_dim, image_dim).data\n",
    "        decoded_data[i] = img\n",
    "\n",
    "    pic = to_img(decoded_data)\n",
    "    save_image(pic, './{}/image_decoded_{}.png'.format(decoded_out_dir,sb_))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 347.414818,
   "position": {
    "height": "469.025px",
    "left": "5.2px",
    "right": "20px",
    "top": "8.9875px",
    "width": "692.275px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
