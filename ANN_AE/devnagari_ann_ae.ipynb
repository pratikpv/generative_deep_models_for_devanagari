{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n",
      "Using GPU: NVIDIA TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print('Running on device:', device)\n",
    "if use_cuda:\n",
    "    print('Using GPU:',\n",
    "          torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/therock/data2/devnagari_data/'\n",
    "\n",
    "expr_name = 'devnagari_ann_ae'\n",
    "model_name = expr_name + '_PyTorch_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images available: 78200\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "# each image in dataset is 32x32 pixels\n",
    "image_dim = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize(image_dim),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "train_data = datasets.ImageFolder(os.path.join(root, 'Train'),\n",
    "                                  transform=train_transform)\n",
    "train_data_len = len(train_data)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "class_names = train_data.classes\n",
    "num_of_classes = len(train_data.classes)\n",
    "\n",
    "print(f'Training images available: {len(train_data)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_out_dir = expr_name + '_devnagari_decoded'\n",
    "if not os.path.exists(decoded_out_dir):\n",
    "    os.mkdir(decoded_out_dir)\n",
    "\n",
    "\n",
    "def to_img(x):\n",
    "    x = x.view(x.size(0), 1, image_dim, image_dim)\n",
    "    return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,image_dim=32):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        in_dim = image_dim * image_dim\n",
    "        l1_per = 0.90\n",
    "        l2_per = 0.80\n",
    "        l3_per = 0.60\n",
    "        l4_per = 0.40\n",
    "        l5_per = 0.30\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_dim, int(in_dim * l1_per)),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(int(in_dim * l1_per), int(in_dim * l2_per)),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(int(in_dim * l2_per), int(in_dim * l3_per)),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(int(in_dim * l3_per), int(in_dim * l4_per)),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(int(in_dim * l4_per), int(in_dim * l5_per)),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, image_dim=32):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "        in_dim = image_dim * image_dim\n",
    "        l1_per = 0.90\n",
    "        l2_per = 0.80\n",
    "        l3_per = 0.60\n",
    "        l4_per = 0.40\n",
    "        l5_per = 0.30\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(int(in_dim * l5_per),\n",
    "                      int(in_dim * l4_per)), nn.ReLU(True),\n",
    "            nn.Linear(int(in_dim * l4_per),\n",
    "                      int(in_dim * l3_per)), nn.ReLU(True),\n",
    "            nn.Linear(int(in_dim * l3_per),\n",
    "                      int(in_dim * l2_per)), nn.ReLU(True),\n",
    "            nn.Linear(int(in_dim * l2_per), int(in_dim * l1_per)),\n",
    "            nn.ReLU(True), nn.Linear(int(in_dim * l1_per), in_dim), nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, encd, decd):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = encd\n",
    "        self.decoder = decd\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder(\n",
      "  (encoder): Encoder(\n",
      "    (encoder): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=921, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=921, out_features=819, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): Linear(in_features=819, out_features=614, bias=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Linear(in_features=614, out_features=409, bias=True)\n",
      "      (7): ReLU(inplace=True)\n",
      "      (8): Linear(in_features=409, out_features=307, bias=True)\n",
      "      (9): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=307, out_features=409, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=409, out_features=614, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): Linear(in_features=614, out_features=819, bias=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Linear(in_features=819, out_features=921, bias=True)\n",
      "      (7): ReLU(inplace=True)\n",
      "      (8): Linear(in_features=921, out_features=1024, bias=True)\n",
      "      (9): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(image_dim=image_dim)\n",
    "decoder = Decoder(image_dim=image_dim)\n",
    "\n",
    "model = AutoEncoder(encoder, decoder)\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "print(model)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 : 943104\n",
      " 1 : 921\n",
      " 2 : 754299\n",
      " 3 : 819\n",
      " 4 : 502866\n",
      " 5 : 614\n",
      " 6 : 251126\n",
      " 7 : 409\n",
      " 8 : 125563\n",
      " 9 : 307\n",
      "10 : 125563\n",
      "11 : 409\n",
      "12 : 251126\n",
      "13 : 614\n",
      "14 : 502866\n",
      "15 : 819\n",
      "16 : 754299\n",
      "17 : 921\n",
      "18 : 943104\n",
      "19 : 1024\n",
      "==========\n",
      "5160773\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for i, item in enumerate(params):\n",
    "        print(f'{i:2} : {item:}')\n",
    "    print(f'==========\\n{sum(params):>6}')\n",
    "\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 batch:  100 [ 25600/78200]  loss: 0.11276305\n",
      "epoch:  0 batch:  200 [ 51200/78200]  loss: 0.10967003\n",
      "epoch:  0 batch:  300 [ 76800/78200]  loss: 0.10803904\n",
      "epoch:  1 batch:  100 [ 25600/78200]  loss: 0.10070109\n",
      "epoch:  1 batch:  200 [ 51200/78200]  loss: 0.09205042\n",
      "epoch:  1 batch:  300 [ 76800/78200]  loss: 0.08648089\n",
      "epoch:  2 batch:  100 [ 25600/78200]  loss: 0.08245252\n",
      "epoch:  2 batch:  200 [ 51200/78200]  loss: 0.07728819\n",
      "epoch:  2 batch:  300 [ 76800/78200]  loss: 0.07226260\n",
      "epoch:  3 batch:  100 [ 25600/78200]  loss: 0.07143919\n",
      "epoch:  3 batch:  200 [ 51200/78200]  loss: 0.06807701\n",
      "epoch:  3 batch:  300 [ 76800/78200]  loss: 0.06620750\n",
      "epoch:  4 batch:  100 [ 25600/78200]  loss: 0.06423061\n",
      "epoch:  4 batch:  200 [ 51200/78200]  loss: 0.06100991\n",
      "epoch:  4 batch:  300 [ 76800/78200]  loss: 0.05948922\n",
      "epoch:  5 batch:  100 [ 25600/78200]  loss: 0.05794276\n",
      "epoch:  5 batch:  200 [ 51200/78200]  loss: 0.05698378\n",
      "epoch:  5 batch:  300 [ 76800/78200]  loss: 0.05547250\n",
      "epoch:  6 batch:  100 [ 25600/78200]  loss: 0.05328052\n",
      "epoch:  6 batch:  200 [ 51200/78200]  loss: 0.05462772\n",
      "epoch:  6 batch:  300 [ 76800/78200]  loss: 0.05182610\n",
      "epoch:  7 batch:  100 [ 25600/78200]  loss: 0.05108415\n",
      "epoch:  7 batch:  200 [ 51200/78200]  loss: 0.04934056\n",
      "epoch:  7 batch:  300 [ 76800/78200]  loss: 0.04990510\n",
      "epoch:  8 batch:  100 [ 25600/78200]  loss: 0.04713008\n",
      "epoch:  8 batch:  200 [ 51200/78200]  loss: 0.04887056\n",
      "epoch:  8 batch:  300 [ 76800/78200]  loss: 0.04611586\n",
      "epoch:  9 batch:  100 [ 25600/78200]  loss: 0.04452666\n",
      "epoch:  9 batch:  200 [ 51200/78200]  loss: 0.04474388\n",
      "epoch:  9 batch:  300 [ 76800/78200]  loss: 0.04256168\n",
      "epoch: 10 batch:  100 [ 25600/78200]  loss: 0.04262744\n",
      "epoch: 10 batch:  200 [ 51200/78200]  loss: 0.04212980\n",
      "epoch: 10 batch:  300 [ 76800/78200]  loss: 0.04163913\n",
      "epoch: 11 batch:  100 [ 25600/78200]  loss: 0.04201454\n",
      "epoch: 11 batch:  200 [ 51200/78200]  loss: 0.04048357\n",
      "epoch: 11 batch:  300 [ 76800/78200]  loss: 0.04005455\n",
      "epoch: 12 batch:  100 [ 25600/78200]  loss: 0.04126698\n",
      "epoch: 12 batch:  200 [ 51200/78200]  loss: 0.03925560\n",
      "epoch: 12 batch:  300 [ 76800/78200]  loss: 0.04087359\n",
      "epoch: 13 batch:  100 [ 25600/78200]  loss: 0.03820978\n",
      "epoch: 13 batch:  200 [ 51200/78200]  loss: 0.04014121\n",
      "epoch: 13 batch:  300 [ 76800/78200]  loss: 0.03792305\n",
      "epoch: 14 batch:  100 [ 25600/78200]  loss: 0.03701860\n",
      "epoch: 14 batch:  200 [ 51200/78200]  loss: 0.03638337\n",
      "epoch: 14 batch:  300 [ 76800/78200]  loss: 0.03808869\n",
      "epoch: 15 batch:  100 [ 25600/78200]  loss: 0.03536167\n",
      "epoch: 15 batch:  200 [ 51200/78200]  loss: 0.03492334\n",
      "epoch: 15 batch:  300 [ 76800/78200]  loss: 0.03542311\n",
      "epoch: 16 batch:  100 [ 25600/78200]  loss: 0.03593641\n",
      "epoch: 16 batch:  200 [ 51200/78200]  loss: 0.03385796\n",
      "epoch: 16 batch:  300 [ 76800/78200]  loss: 0.03624407\n",
      "epoch: 17 batch:  100 [ 25600/78200]  loss: 0.03425286\n",
      "epoch: 17 batch:  200 [ 51200/78200]  loss: 0.03480808\n",
      "epoch: 17 batch:  300 [ 76800/78200]  loss: 0.03400552\n",
      "epoch: 18 batch:  100 [ 25600/78200]  loss: 0.03291324\n",
      "epoch: 18 batch:  200 [ 51200/78200]  loss: 0.03303368\n",
      "epoch: 18 batch:  300 [ 76800/78200]  loss: 0.03314964\n",
      "epoch: 19 batch:  100 [ 25600/78200]  loss: 0.03359717\n",
      "epoch: 19 batch:  200 [ 51200/78200]  loss: 0.03352278\n",
      "epoch: 19 batch:  300 [ 76800/78200]  loss: 0.03309444\n",
      "epoch: 20 batch:  100 [ 25600/78200]  loss: 0.03231854\n",
      "epoch: 20 batch:  200 [ 51200/78200]  loss: 0.03235926\n",
      "epoch: 20 batch:  300 [ 76800/78200]  loss: 0.03133900\n",
      "epoch: 21 batch:  100 [ 25600/78200]  loss: 0.03171634\n",
      "epoch: 21 batch:  200 [ 51200/78200]  loss: 0.03115546\n",
      "epoch: 21 batch:  300 [ 76800/78200]  loss: 0.03021947\n",
      "epoch: 22 batch:  100 [ 25600/78200]  loss: 0.03234793\n",
      "epoch: 22 batch:  200 [ 51200/78200]  loss: 0.03123092\n",
      "epoch: 22 batch:  300 [ 76800/78200]  loss: 0.03184230\n",
      "epoch: 23 batch:  100 [ 25600/78200]  loss: 0.02901932\n",
      "epoch: 23 batch:  200 [ 51200/78200]  loss: 0.03037648\n",
      "epoch: 23 batch:  300 [ 76800/78200]  loss: 0.03111367\n",
      "epoch: 24 batch:  100 [ 25600/78200]  loss: 0.03047554\n",
      "epoch: 24 batch:  200 [ 51200/78200]  loss: 0.02909382\n",
      "epoch: 24 batch:  300 [ 76800/78200]  loss: 0.02904260\n",
      "epoch: 25 batch:  100 [ 25600/78200]  loss: 0.02850482\n",
      "epoch: 25 batch:  200 [ 51200/78200]  loss: 0.02869822\n",
      "epoch: 25 batch:  300 [ 76800/78200]  loss: 0.02889968\n",
      "epoch: 26 batch:  100 [ 25600/78200]  loss: 0.02914469\n",
      "epoch: 26 batch:  200 [ 51200/78200]  loss: 0.02771276\n",
      "epoch: 26 batch:  300 [ 76800/78200]  loss: 0.02930494\n",
      "epoch: 27 batch:  100 [ 25600/78200]  loss: 0.02780833\n",
      "epoch: 27 batch:  200 [ 51200/78200]  loss: 0.02743333\n",
      "epoch: 27 batch:  300 [ 76800/78200]  loss: 0.02877148\n",
      "epoch: 28 batch:  100 [ 25600/78200]  loss: 0.02738691\n",
      "epoch: 28 batch:  200 [ 51200/78200]  loss: 0.02772537\n",
      "epoch: 28 batch:  300 [ 76800/78200]  loss: 0.02755343\n",
      "epoch: 29 batch:  100 [ 25600/78200]  loss: 0.02639179\n",
      "epoch: 29 batch:  200 [ 51200/78200]  loss: 0.02602928\n",
      "epoch: 29 batch:  300 [ 76800/78200]  loss: 0.02674332\n",
      "epoch: 30 batch:  100 [ 25600/78200]  loss: 0.02624437\n",
      "epoch: 30 batch:  200 [ 51200/78200]  loss: 0.02630013\n",
      "epoch: 30 batch:  300 [ 76800/78200]  loss: 0.02701141\n",
      "epoch: 31 batch:  100 [ 25600/78200]  loss: 0.02626320\n",
      "epoch: 31 batch:  200 [ 51200/78200]  loss: 0.02719586\n",
      "epoch: 31 batch:  300 [ 76800/78200]  loss: 0.02673128\n",
      "epoch: 32 batch:  100 [ 25600/78200]  loss: 0.02495592\n",
      "epoch: 32 batch:  200 [ 51200/78200]  loss: 0.02651156\n",
      "epoch: 32 batch:  300 [ 76800/78200]  loss: 0.02644286\n",
      "epoch: 33 batch:  100 [ 25600/78200]  loss: 0.02550252\n",
      "epoch: 33 batch:  200 [ 51200/78200]  loss: 0.02624929\n",
      "epoch: 33 batch:  300 [ 76800/78200]  loss: 0.02579917\n",
      "epoch: 34 batch:  100 [ 25600/78200]  loss: 0.02486143\n",
      "epoch: 34 batch:  200 [ 51200/78200]  loss: 0.02647669\n",
      "epoch: 34 batch:  300 [ 76800/78200]  loss: 0.02558336\n",
      "epoch: 35 batch:  100 [ 25600/78200]  loss: 0.02497805\n",
      "epoch: 35 batch:  200 [ 51200/78200]  loss: 0.02593799\n",
      "epoch: 35 batch:  300 [ 76800/78200]  loss: 0.02569219\n",
      "epoch: 36 batch:  100 [ 25600/78200]  loss: 0.02571908\n",
      "epoch: 36 batch:  200 [ 51200/78200]  loss: 0.02516900\n",
      "epoch: 36 batch:  300 [ 76800/78200]  loss: 0.02512887\n",
      "epoch: 37 batch:  100 [ 25600/78200]  loss: 0.02429273\n",
      "epoch: 37 batch:  200 [ 51200/78200]  loss: 0.02404559\n",
      "epoch: 37 batch:  300 [ 76800/78200]  loss: 0.02463613\n",
      "epoch: 38 batch:  100 [ 25600/78200]  loss: 0.02363269\n",
      "epoch: 38 batch:  200 [ 51200/78200]  loss: 0.02374183\n",
      "epoch: 38 batch:  300 [ 76800/78200]  loss: 0.02396351\n",
      "epoch: 39 batch:  100 [ 25600/78200]  loss: 0.02467624\n",
      "epoch: 39 batch:  200 [ 51200/78200]  loss: 0.02329503\n",
      "epoch: 39 batch:  300 [ 76800/78200]  loss: 0.02446138\n",
      "epoch: 40 batch:  100 [ 25600/78200]  loss: 0.02272606\n",
      "epoch: 40 batch:  200 [ 51200/78200]  loss: 0.02310935\n",
      "epoch: 40 batch:  300 [ 76800/78200]  loss: 0.02435288\n",
      "epoch: 41 batch:  100 [ 25600/78200]  loss: 0.02412469\n",
      "epoch: 41 batch:  200 [ 51200/78200]  loss: 0.02361010\n",
      "epoch: 41 batch:  300 [ 76800/78200]  loss: 0.02276354\n",
      "epoch: 42 batch:  100 [ 25600/78200]  loss: 0.02354857\n",
      "epoch: 42 batch:  200 [ 51200/78200]  loss: 0.02346855\n",
      "epoch: 42 batch:  300 [ 76800/78200]  loss: 0.02455379\n",
      "epoch: 43 batch:  100 [ 25600/78200]  loss: 0.02334228\n",
      "epoch: 43 batch:  200 [ 51200/78200]  loss: 0.02361000\n",
      "epoch: 43 batch:  300 [ 76800/78200]  loss: 0.02338433\n",
      "epoch: 44 batch:  100 [ 25600/78200]  loss: 0.02324830\n",
      "epoch: 44 batch:  200 [ 51200/78200]  loss: 0.02391092\n",
      "epoch: 44 batch:  300 [ 76800/78200]  loss: 0.02282567\n",
      "epoch: 45 batch:  100 [ 25600/78200]  loss: 0.02254129\n",
      "epoch: 45 batch:  200 [ 51200/78200]  loss: 0.02292190\n",
      "epoch: 45 batch:  300 [ 76800/78200]  loss: 0.02189876\n",
      "epoch: 46 batch:  100 [ 25600/78200]  loss: 0.02223456\n",
      "epoch: 46 batch:  200 [ 51200/78200]  loss: 0.02346054\n",
      "epoch: 46 batch:  300 [ 76800/78200]  loss: 0.02218571\n",
      "epoch: 47 batch:  100 [ 25600/78200]  loss: 0.02213562\n",
      "epoch: 47 batch:  200 [ 51200/78200]  loss: 0.02190676\n",
      "epoch: 47 batch:  300 [ 76800/78200]  loss: 0.02342656\n",
      "epoch: 48 batch:  100 [ 25600/78200]  loss: 0.02203825\n",
      "epoch: 48 batch:  200 [ 51200/78200]  loss: 0.02229855\n",
      "epoch: 48 batch:  300 [ 76800/78200]  loss: 0.02262615\n",
      "epoch: 49 batch:  100 [ 25600/78200]  loss: 0.02173622\n",
      "epoch: 49 batch:  200 [ 51200/78200]  loss: 0.02170891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49 batch:  300 [ 76800/78200]  loss: 0.02240000\n",
      "epoch: 50 batch:  100 [ 25600/78200]  loss: 0.02175862\n",
      "epoch: 50 batch:  200 [ 51200/78200]  loss: 0.02214536\n",
      "epoch: 50 batch:  300 [ 76800/78200]  loss: 0.02152604\n",
      "epoch: 51 batch:  100 [ 25600/78200]  loss: 0.02122615\n",
      "epoch: 51 batch:  200 [ 51200/78200]  loss: 0.02139453\n",
      "epoch: 51 batch:  300 [ 76800/78200]  loss: 0.02221837\n",
      "epoch: 52 batch:  100 [ 25600/78200]  loss: 0.02081285\n",
      "epoch: 52 batch:  200 [ 51200/78200]  loss: 0.02162021\n",
      "epoch: 52 batch:  300 [ 76800/78200]  loss: 0.02214222\n",
      "epoch: 53 batch:  100 [ 25600/78200]  loss: 0.02065838\n",
      "epoch: 53 batch:  200 [ 51200/78200]  loss: 0.02097954\n",
      "epoch: 53 batch:  300 [ 76800/78200]  loss: 0.02156075\n",
      "epoch: 54 batch:  100 [ 25600/78200]  loss: 0.02096589\n",
      "epoch: 54 batch:  200 [ 51200/78200]  loss: 0.02084119\n",
      "epoch: 54 batch:  300 [ 76800/78200]  loss: 0.02255558\n",
      "epoch: 55 batch:  100 [ 25600/78200]  loss: 0.02034922\n",
      "epoch: 55 batch:  200 [ 51200/78200]  loss: 0.02135447\n",
      "epoch: 55 batch:  300 [ 76800/78200]  loss: 0.02151854\n",
      "epoch: 56 batch:  100 [ 25600/78200]  loss: 0.02093673\n",
      "epoch: 56 batch:  200 [ 51200/78200]  loss: 0.02068184\n",
      "epoch: 56 batch:  300 [ 76800/78200]  loss: 0.02139636\n",
      "epoch: 57 batch:  100 [ 25600/78200]  loss: 0.02088423\n",
      "epoch: 57 batch:  200 [ 51200/78200]  loss: 0.02023950\n",
      "epoch: 57 batch:  300 [ 76800/78200]  loss: 0.02083765\n",
      "epoch: 58 batch:  100 [ 25600/78200]  loss: 0.02048638\n",
      "epoch: 58 batch:  200 [ 51200/78200]  loss: 0.01963336\n",
      "epoch: 58 batch:  300 [ 76800/78200]  loss: 0.02035530\n",
      "epoch: 59 batch:  100 [ 25600/78200]  loss: 0.01970888\n",
      "epoch: 59 batch:  200 [ 51200/78200]  loss: 0.02153600\n",
      "epoch: 59 batch:  300 [ 76800/78200]  loss: 0.02115313\n",
      "epoch: 60 batch:  100 [ 25600/78200]  loss: 0.02024996\n",
      "epoch: 60 batch:  200 [ 51200/78200]  loss: 0.02092001\n",
      "epoch: 60 batch:  300 [ 76800/78200]  loss: 0.01977554\n",
      "epoch: 61 batch:  100 [ 25600/78200]  loss: 0.02059809\n",
      "epoch: 61 batch:  200 [ 51200/78200]  loss: 0.02083136\n",
      "epoch: 61 batch:  300 [ 76800/78200]  loss: 0.02114186\n",
      "epoch: 62 batch:  100 [ 25600/78200]  loss: 0.01982043\n",
      "epoch: 62 batch:  200 [ 51200/78200]  loss: 0.01922951\n",
      "epoch: 62 batch:  300 [ 76800/78200]  loss: 0.02004285\n",
      "epoch: 63 batch:  100 [ 25600/78200]  loss: 0.01967225\n",
      "epoch: 63 batch:  200 [ 51200/78200]  loss: 0.02036501\n",
      "epoch: 63 batch:  300 [ 76800/78200]  loss: 0.02037916\n",
      "epoch: 64 batch:  100 [ 25600/78200]  loss: 0.01996006\n",
      "epoch: 64 batch:  200 [ 51200/78200]  loss: 0.02001999\n",
      "epoch: 64 batch:  300 [ 76800/78200]  loss: 0.01912361\n",
      "epoch: 65 batch:  100 [ 25600/78200]  loss: 0.01967863\n",
      "epoch: 65 batch:  200 [ 51200/78200]  loss: 0.01927568\n",
      "epoch: 65 batch:  300 [ 76800/78200]  loss: 0.02054906\n",
      "epoch: 66 batch:  100 [ 25600/78200]  loss: 0.01967370\n",
      "epoch: 66 batch:  200 [ 51200/78200]  loss: 0.01922650\n",
      "epoch: 66 batch:  300 [ 76800/78200]  loss: 0.01986450\n",
      "epoch: 67 batch:  100 [ 25600/78200]  loss: 0.01904335\n",
      "epoch: 67 batch:  200 [ 51200/78200]  loss: 0.01958056\n",
      "epoch: 67 batch:  300 [ 76800/78200]  loss: 0.01971721\n",
      "epoch: 68 batch:  100 [ 25600/78200]  loss: 0.01892084\n",
      "epoch: 68 batch:  200 [ 51200/78200]  loss: 0.01925992\n",
      "epoch: 68 batch:  300 [ 76800/78200]  loss: 0.01979249\n",
      "epoch: 69 batch:  100 [ 25600/78200]  loss: 0.01942560\n",
      "epoch: 69 batch:  200 [ 51200/78200]  loss: 0.01888557\n",
      "epoch: 69 batch:  300 [ 76800/78200]  loss: 0.01844414\n",
      "epoch: 70 batch:  100 [ 25600/78200]  loss: 0.01916110\n",
      "epoch: 70 batch:  200 [ 51200/78200]  loss: 0.01984851\n",
      "epoch: 70 batch:  300 [ 76800/78200]  loss: 0.01921667\n",
      "epoch: 71 batch:  100 [ 25600/78200]  loss: 0.01908435\n",
      "epoch: 71 batch:  200 [ 51200/78200]  loss: 0.01923847\n",
      "epoch: 71 batch:  300 [ 76800/78200]  loss: 0.01955727\n",
      "epoch: 72 batch:  100 [ 25600/78200]  loss: 0.01938055\n",
      "epoch: 72 batch:  200 [ 51200/78200]  loss: 0.01840459\n",
      "epoch: 72 batch:  300 [ 76800/78200]  loss: 0.01844417\n",
      "epoch: 73 batch:  100 [ 25600/78200]  loss: 0.01946036\n",
      "epoch: 73 batch:  200 [ 51200/78200]  loss: 0.01916998\n",
      "epoch: 73 batch:  300 [ 76800/78200]  loss: 0.01897389\n",
      "epoch: 74 batch:  100 [ 25600/78200]  loss: 0.01819276\n",
      "epoch: 74 batch:  200 [ 51200/78200]  loss: 0.01869600\n",
      "epoch: 74 batch:  300 [ 76800/78200]  loss: 0.01872966\n",
      "epoch: 75 batch:  100 [ 25600/78200]  loss: 0.01882182\n",
      "epoch: 75 batch:  200 [ 51200/78200]  loss: 0.01869955\n",
      "epoch: 75 batch:  300 [ 76800/78200]  loss: 0.01932685\n",
      "epoch: 76 batch:  100 [ 25600/78200]  loss: 0.01825656\n",
      "epoch: 76 batch:  200 [ 51200/78200]  loss: 0.01844827\n",
      "epoch: 76 batch:  300 [ 76800/78200]  loss: 0.01817393\n",
      "epoch: 77 batch:  100 [ 25600/78200]  loss: 0.01844636\n",
      "epoch: 77 batch:  200 [ 51200/78200]  loss: 0.01935542\n",
      "epoch: 77 batch:  300 [ 76800/78200]  loss: 0.01901484\n",
      "epoch: 78 batch:  100 [ 25600/78200]  loss: 0.01949636\n",
      "epoch: 78 batch:  200 [ 51200/78200]  loss: 0.01883715\n",
      "epoch: 78 batch:  300 [ 76800/78200]  loss: 0.01859759\n",
      "epoch: 79 batch:  100 [ 25600/78200]  loss: 0.01813973\n",
      "epoch: 79 batch:  200 [ 51200/78200]  loss: 0.01830868\n",
      "epoch: 79 batch:  300 [ 76800/78200]  loss: 0.01912975\n",
      "epoch: 80 batch:  100 [ 25600/78200]  loss: 0.01842308\n",
      "epoch: 80 batch:  200 [ 51200/78200]  loss: 0.01826803\n",
      "epoch: 80 batch:  300 [ 76800/78200]  loss: 0.01872139\n",
      "epoch: 81 batch:  100 [ 25600/78200]  loss: 0.01864608\n",
      "epoch: 81 batch:  200 [ 51200/78200]  loss: 0.01838925\n",
      "epoch: 81 batch:  300 [ 76800/78200]  loss: 0.01851109\n",
      "epoch: 82 batch:  100 [ 25600/78200]  loss: 0.01770190\n",
      "epoch: 82 batch:  200 [ 51200/78200]  loss: 0.01904961\n",
      "epoch: 82 batch:  300 [ 76800/78200]  loss: 0.01882811\n",
      "epoch: 83 batch:  100 [ 25600/78200]  loss: 0.01838998\n",
      "epoch: 83 batch:  200 [ 51200/78200]  loss: 0.01852837\n",
      "epoch: 83 batch:  300 [ 76800/78200]  loss: 0.01760325\n",
      "epoch: 84 batch:  100 [ 25600/78200]  loss: 0.01782971\n",
      "epoch: 84 batch:  200 [ 51200/78200]  loss: 0.01925296\n",
      "epoch: 84 batch:  300 [ 76800/78200]  loss: 0.01788720\n",
      "epoch: 85 batch:  100 [ 25600/78200]  loss: 0.01752170\n",
      "epoch: 85 batch:  200 [ 51200/78200]  loss: 0.01778167\n",
      "epoch: 85 batch:  300 [ 76800/78200]  loss: 0.01849390\n",
      "epoch: 86 batch:  100 [ 25600/78200]  loss: 0.01771866\n",
      "epoch: 86 batch:  200 [ 51200/78200]  loss: 0.01890591\n",
      "epoch: 86 batch:  300 [ 76800/78200]  loss: 0.01808364\n",
      "epoch: 87 batch:  100 [ 25600/78200]  loss: 0.01824145\n",
      "epoch: 87 batch:  200 [ 51200/78200]  loss: 0.01835545\n",
      "epoch: 87 batch:  300 [ 76800/78200]  loss: 0.01869304\n",
      "epoch: 88 batch:  100 [ 25600/78200]  loss: 0.01811173\n",
      "epoch: 88 batch:  200 [ 51200/78200]  loss: 0.01763161\n",
      "epoch: 88 batch:  300 [ 76800/78200]  loss: 0.01841356\n",
      "epoch: 89 batch:  100 [ 25600/78200]  loss: 0.01809271\n",
      "epoch: 89 batch:  200 [ 51200/78200]  loss: 0.01774165\n",
      "epoch: 89 batch:  300 [ 76800/78200]  loss: 0.01770638\n",
      "epoch: 90 batch:  100 [ 25600/78200]  loss: 0.01789857\n",
      "epoch: 90 batch:  200 [ 51200/78200]  loss: 0.01792443\n",
      "epoch: 90 batch:  300 [ 76800/78200]  loss: 0.01745496\n",
      "epoch: 91 batch:  100 [ 25600/78200]  loss: 0.01816409\n",
      "epoch: 91 batch:  200 [ 51200/78200]  loss: 0.01769709\n",
      "epoch: 91 batch:  300 [ 76800/78200]  loss: 0.01834697\n",
      "epoch: 92 batch:  100 [ 25600/78200]  loss: 0.01789727\n",
      "epoch: 92 batch:  200 [ 51200/78200]  loss: 0.01668113\n",
      "epoch: 92 batch:  300 [ 76800/78200]  loss: 0.01788659\n",
      "epoch: 93 batch:  100 [ 25600/78200]  loss: 0.01770035\n",
      "epoch: 93 batch:  200 [ 51200/78200]  loss: 0.01763977\n",
      "epoch: 93 batch:  300 [ 76800/78200]  loss: 0.01800333\n",
      "epoch: 94 batch:  100 [ 25600/78200]  loss: 0.01726198\n",
      "epoch: 94 batch:  200 [ 51200/78200]  loss: 0.01755704\n",
      "epoch: 94 batch:  300 [ 76800/78200]  loss: 0.01744729\n",
      "epoch: 95 batch:  100 [ 25600/78200]  loss: 0.01706463\n",
      "epoch: 95 batch:  200 [ 51200/78200]  loss: 0.01750631\n",
      "epoch: 95 batch:  300 [ 76800/78200]  loss: 0.01801766\n",
      "epoch: 96 batch:  100 [ 25600/78200]  loss: 0.01745236\n",
      "epoch: 96 batch:  200 [ 51200/78200]  loss: 0.01822011\n",
      "epoch: 96 batch:  300 [ 76800/78200]  loss: 0.01858483\n",
      "epoch: 97 batch:  100 [ 25600/78200]  loss: 0.01691997\n",
      "epoch: 97 batch:  200 [ 51200/78200]  loss: 0.01702554\n",
      "epoch: 97 batch:  300 [ 76800/78200]  loss: 0.01745441\n",
      "epoch: 98 batch:  100 [ 25600/78200]  loss: 0.01811521\n",
      "epoch: 98 batch:  200 [ 51200/78200]  loss: 0.01686197\n",
      "epoch: 98 batch:  300 [ 76800/78200]  loss: 0.01814236\n",
      "epoch: 99 batch:  100 [ 25600/78200]  loss: 0.01651933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 99 batch:  200 [ 51200/78200]  loss: 0.01659441\n",
      "epoch: 99 batch:  300 [ 76800/78200]  loss: 0.01806757\n"
     ]
    }
   ],
   "source": [
    "lowest_loss = float(\"inf\")\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for b, (X_train, y_train) in enumerate(train_loader):\n",
    "\n",
    "        b += 1\n",
    "\n",
    "        X_train = X_train.reshape(X_train.size(0), -1)\n",
    "        X_train = X_train.to(device)\n",
    "\n",
    "        # Apply the model\n",
    "        output = model(X_train)\n",
    "        loss = criterion(output, X_train)  # check loss with X_train itself\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if b % 100 == 0:\n",
    "            print(\n",
    "                f'epoch: {epoch:2} batch: {b:4} [{batch_size*b:6}/{train_data_len}]  '\n",
    "                + f'loss: {loss.item():.8f}')\n",
    "\n",
    "        if loss.item() < lowest_loss:\n",
    "            lowest_loss = loss.item()\n",
    "            torch.save(model.state_dict(), model_name)   \n",
    "            #print(f\"saved model\")\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "        save_image(pic, './{}/image_{}.png'.format(decoded_out_dir, epoch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = image_dim * image_dim\n",
    "latent_dim = int(in_dim * 0.30)\n",
    "\n",
    "sample_batches = 20\n",
    "decoded_data = torch.FloatTensor(batch_size,1,image_dim,image_dim)\n",
    "\n",
    "encoder_test = Encoder(image_dim=image_dim)\n",
    "decoder_test = Decoder(image_dim=image_dim)\n",
    "\n",
    "model_test = AutoEncoder(encoder_test, decoder_test)\n",
    "model_test.load_state_dict = torch.load(model_name)\n",
    "model_test.eval()\n",
    "\n",
    "if use_cuda:\n",
    "    model_test = model_test.cuda()\n",
    "\n",
    "for sb_ in range(sample_batches):\n",
    "    for i in range(batch_size):\n",
    "        z = torch.randn(1, latent_dim).to(device)\n",
    "        reconstructed_img = model_test.decoder(z).to('cpu')\n",
    "        img = reconstructed_img.view(image_dim, image_dim).data\n",
    "        decoded_data[i] = img\n",
    "\n",
    "    pic = to_img(decoded_data)\n",
    "    save_image(pic, './{}/image_decoded_{}.png'.format(decoded_out_dir,sb_))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 347.414818,
   "position": {
    "height": "369.233px",
    "left": "7.2074px",
    "right": "20px",
    "top": "8.99999px",
    "width": "663.494px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
