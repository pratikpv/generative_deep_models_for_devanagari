{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n",
      "Using GPU: NVIDIA TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print('Running on device:', device)\n",
    "if use_cuda:\n",
    "    print('Using GPU:',\n",
    "          torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/therock/data2/devnagari_data/'\n",
    "\n",
    "expr_name = 'devnagari_cnn_ae'\n",
    "model_name = expr_name + '_PyTorch_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images available: 78200\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "# each image in dataset is 32x32 pixels\n",
    "image_dim = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize(image_dim),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(os.path.join(root, 'Train'),\n",
    "                                  transform=train_transform)\n",
    "\n",
    "train_data_len = len(train_data)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class_names = train_data.classes\n",
    "num_of_classes = len(train_data.classes)\n",
    "\n",
    "print(f'Training images available: {len(train_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_out_dir = expr_name + '_decoded'\n",
    "if not os.path.exists(decoded_out_dir):\n",
    "    os.mkdir(decoded_out_dir)\n",
    "\n",
    "\n",
    "def to_img(x):\n",
    "    x = x.view(x.size(0), 1, 32, 32)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_conv_out(n=1, p=1, f=1, s=1):\n",
    "        return int(((n + 2 * p - f) / s) + 1)\n",
    "\n",
    "def calc_deconv_out(n=1, p=1512, f=1, s=1):\n",
    "        return int(s * (n - 1) + f - 2 * p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 30\n",
      "2: 15\n",
      "3: 11\n",
      "4: 5\n",
      "de 5: 7\n",
      "de 6: 15\n",
      "de 7: 31\n"
     ]
    }
   ],
   "source": [
    "conv1 = calc_conv_out(n=32, f=3, s=1, p=0)\n",
    "mp1 = calc_conv_out(n=conv1, f=2, s=2, p=0)\n",
    "conv2 = calc_conv_out(n=mp1, f=5, s=1, p=0)\n",
    "mp2 = calc_conv_out(n=conv2, f=2, s=2, p=0)\n",
    "deconv1 = calc_deconv_out(n=mp2, f=3, s=1, p=0)\n",
    "deconv2 = calc_deconv_out(n=deconv1, f=3, s=2, p=0)\n",
    "deconv3 = calc_deconv_out(n=deconv2, f=3, s=2, p=0)\n",
    "\n",
    "\n",
    "print('1:', conv1)\n",
    "print('2:', mp1)\n",
    "print('3:', conv2)\n",
    "print('4:', mp2)\n",
    "print('de 5:', deconv1)\n",
    "print('de 6:', deconv2)\n",
    "print('de 7:', deconv3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self,latent_dim=64):\n",
    "\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=1, padding=0),  # b, 32, 30,30\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # b, 32, 15, 15\n",
    "            nn.Conv2d(16, 8, 5, stride=1, padding=0),  # b, 16, 11, 11\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2)  # b, 8, 5, 5\n",
    "        )\n",
    "        \n",
    "        self.l = nn.Linear( 8*5*5, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(f\"encoder in {x.shape}\")\n",
    "        x = self.encoder(x)\n",
    "        #print(f\"encoded {x.shape}\")\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        #print(f\"encoded flat{x.shape}\")\n",
    "        x = self.l(x)\n",
    "        #print(f\"encoded out{x.shape}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNDecoder(nn.Module):\n",
    "    def __init__(self,latent_dim=64):\n",
    "\n",
    "        super(CNNDecoder, self).__init__()\n",
    "\n",
    "        self.l = nn.Linear(latent_dim, 8*5*5)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 4, 3, stride=1, padding=0),  # b, 12, 11,11\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(4, 2, 3, stride=2, padding=0),  # b, 6, 17, 17\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(2, 1, 3, stride=2, padding=0,\n",
    "                               output_padding=1),  # b, 1, 32, 32\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(f\"within decoder in {x.shape}\")\n",
    "        x = self.l(x)\n",
    "        x = x.view(x.shape[0],8,5,5)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNAutoencoder(nn.Module):\n",
    "    def __init__(self, encd, decd,latent_dim=64):\n",
    "        super(CNNAutoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = encd\n",
    "        self.decoder = decd\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(f\"auto in {x.shape}\")\n",
    "        x = self.encoder(x)\n",
    "        #print(f\"encoder out {x.shape}\")\n",
    "        x = self.decoder(x)\n",
    "        #print(f\"decoder out {x.shape}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim=256\n",
    "encoder = CNNEncoder(latent_dim=latent_dim)\n",
    "decoder = CNNDecoder(latent_dim=latent_dim)\n",
    "\n",
    "model = CNNAutoencoder(encoder, decoder,latent_dim=latent_dim)\n",
    "if use_cuda:\n",
    "    model = model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 : 144\n",
      " 1 : 16\n",
      " 2 : 3200\n",
      " 3 : 8\n",
      " 4 : 51200\n",
      " 5 : 256\n",
      " 6 : 51200\n",
      " 7 : 200\n",
      " 8 : 288\n",
      " 9 : 4\n",
      "10 : 72\n",
      "11 : 2\n",
      "12 : 18\n",
      "13 : 1\n",
      "==========\n",
      "106609\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for i, item in enumerate(params):\n",
    "        print(f'{i:2} : {item:}')\n",
    "    print(f'==========\\n{sum(params):>6}')\n",
    "\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 batch:  100 [ 25600/78200]  loss: 0.15014136\n",
      "epoch:  0 batch:  200 [ 51200/78200]  loss: 0.05642484\n",
      "epoch:  0 batch:  300 [ 76800/78200]  loss: 0.04489509\n",
      "epoch:  1 batch:  100 [ 25600/78200]  loss: 0.03840628\n",
      "epoch:  1 batch:  200 [ 51200/78200]  loss: 0.03448364\n",
      "epoch:  1 batch:  300 [ 76800/78200]  loss: 0.03077650\n",
      "epoch:  2 batch:  100 [ 25600/78200]  loss: 0.02967726\n",
      "epoch:  2 batch:  200 [ 51200/78200]  loss: 0.02822217\n",
      "epoch:  2 batch:  300 [ 76800/78200]  loss: 0.02762332\n",
      "epoch:  3 batch:  100 [ 25600/78200]  loss: 0.02659765\n",
      "epoch:  3 batch:  200 [ 51200/78200]  loss: 0.02664174\n",
      "epoch:  3 batch:  300 [ 76800/78200]  loss: 0.02561073\n",
      "epoch:  4 batch:  100 [ 25600/78200]  loss: 0.02554200\n",
      "epoch:  4 batch:  200 [ 51200/78200]  loss: 0.02487514\n",
      "epoch:  4 batch:  300 [ 76800/78200]  loss: 0.02328489\n",
      "epoch:  5 batch:  100 [ 25600/78200]  loss: 0.02338807\n",
      "epoch:  5 batch:  200 [ 51200/78200]  loss: 0.02256924\n",
      "epoch:  5 batch:  300 [ 76800/78200]  loss: 0.02318635\n",
      "epoch:  6 batch:  100 [ 25600/78200]  loss: 0.02107158\n",
      "epoch:  6 batch:  200 [ 51200/78200]  loss: 0.02182033\n",
      "epoch:  6 batch:  300 [ 76800/78200]  loss: 0.02138082\n",
      "epoch:  7 batch:  100 [ 25600/78200]  loss: 0.02230231\n",
      "epoch:  7 batch:  200 [ 51200/78200]  loss: 0.02159220\n",
      "epoch:  7 batch:  300 [ 76800/78200]  loss: 0.02163673\n",
      "epoch:  8 batch:  100 [ 25600/78200]  loss: 0.02058141\n",
      "epoch:  8 batch:  200 [ 51200/78200]  loss: 0.02084603\n",
      "epoch:  8 batch:  300 [ 76800/78200]  loss: 0.01967510\n",
      "epoch:  9 batch:  100 [ 25600/78200]  loss: 0.02095891\n",
      "epoch:  9 batch:  200 [ 51200/78200]  loss: 0.02037272\n",
      "epoch:  9 batch:  300 [ 76800/78200]  loss: 0.02086081\n",
      "epoch: 10 batch:  100 [ 25600/78200]  loss: 0.02065153\n",
      "epoch: 10 batch:  200 [ 51200/78200]  loss: 0.02016449\n",
      "epoch: 10 batch:  300 [ 76800/78200]  loss: 0.02001867\n",
      "epoch: 11 batch:  100 [ 25600/78200]  loss: 0.02077202\n",
      "epoch: 11 batch:  200 [ 51200/78200]  loss: 0.02059333\n",
      "epoch: 11 batch:  300 [ 76800/78200]  loss: 0.01983679\n",
      "epoch: 12 batch:  100 [ 25600/78200]  loss: 0.02012616\n",
      "epoch: 12 batch:  200 [ 51200/78200]  loss: 0.01926017\n",
      "epoch: 12 batch:  300 [ 76800/78200]  loss: 0.02047319\n",
      "epoch: 13 batch:  100 [ 25600/78200]  loss: 0.01945131\n",
      "epoch: 13 batch:  200 [ 51200/78200]  loss: 0.01983267\n",
      "epoch: 13 batch:  300 [ 76800/78200]  loss: 0.01935634\n",
      "epoch: 14 batch:  100 [ 25600/78200]  loss: 0.01952809\n",
      "epoch: 14 batch:  200 [ 51200/78200]  loss: 0.01841937\n",
      "epoch: 14 batch:  300 [ 76800/78200]  loss: 0.01880362\n",
      "epoch: 15 batch:  100 [ 25600/78200]  loss: 0.01954946\n",
      "epoch: 15 batch:  200 [ 51200/78200]  loss: 0.01844001\n",
      "epoch: 15 batch:  300 [ 76800/78200]  loss: 0.01881804\n",
      "epoch: 16 batch:  100 [ 25600/78200]  loss: 0.01908781\n",
      "epoch: 16 batch:  200 [ 51200/78200]  loss: 0.01904443\n",
      "epoch: 16 batch:  300 [ 76800/78200]  loss: 0.01879222\n",
      "epoch: 17 batch:  100 [ 25600/78200]  loss: 0.01944346\n",
      "epoch: 17 batch:  200 [ 51200/78200]  loss: 0.01983955\n",
      "epoch: 17 batch:  300 [ 76800/78200]  loss: 0.01913551\n",
      "epoch: 18 batch:  100 [ 25600/78200]  loss: 0.01926604\n",
      "epoch: 18 batch:  200 [ 51200/78200]  loss: 0.01832997\n",
      "epoch: 18 batch:  300 [ 76800/78200]  loss: 0.01921463\n",
      "epoch: 19 batch:  100 [ 25600/78200]  loss: 0.01922338\n",
      "epoch: 19 batch:  200 [ 51200/78200]  loss: 0.01854319\n",
      "epoch: 19 batch:  300 [ 76800/78200]  loss: 0.01852255\n",
      "epoch: 20 batch:  100 [ 25600/78200]  loss: 0.01901459\n",
      "epoch: 20 batch:  200 [ 51200/78200]  loss: 0.01885411\n",
      "epoch: 20 batch:  300 [ 76800/78200]  loss: 0.01827565\n",
      "epoch: 21 batch:  100 [ 25600/78200]  loss: 0.01815903\n",
      "epoch: 21 batch:  200 [ 51200/78200]  loss: 0.01843662\n",
      "epoch: 21 batch:  300 [ 76800/78200]  loss: 0.01803779\n",
      "epoch: 22 batch:  100 [ 25600/78200]  loss: 0.01817256\n",
      "epoch: 22 batch:  200 [ 51200/78200]  loss: 0.01840527\n",
      "epoch: 22 batch:  300 [ 76800/78200]  loss: 0.01842002\n",
      "epoch: 23 batch:  100 [ 25600/78200]  loss: 0.01817384\n",
      "epoch: 23 batch:  200 [ 51200/78200]  loss: 0.01841319\n",
      "epoch: 23 batch:  300 [ 76800/78200]  loss: 0.01823026\n",
      "epoch: 24 batch:  100 [ 25600/78200]  loss: 0.01790360\n",
      "epoch: 24 batch:  200 [ 51200/78200]  loss: 0.01863788\n",
      "epoch: 24 batch:  300 [ 76800/78200]  loss: 0.01829980\n",
      "epoch: 25 batch:  100 [ 25600/78200]  loss: 0.01805223\n",
      "epoch: 25 batch:  200 [ 51200/78200]  loss: 0.01807630\n",
      "epoch: 25 batch:  300 [ 76800/78200]  loss: 0.01797493\n",
      "epoch: 26 batch:  100 [ 25600/78200]  loss: 0.01824768\n",
      "epoch: 26 batch:  200 [ 51200/78200]  loss: 0.01736095\n",
      "epoch: 26 batch:  300 [ 76800/78200]  loss: 0.01821464\n",
      "epoch: 27 batch:  100 [ 25600/78200]  loss: 0.01724964\n",
      "epoch: 27 batch:  200 [ 51200/78200]  loss: 0.01777963\n",
      "epoch: 27 batch:  300 [ 76800/78200]  loss: 0.01822869\n",
      "epoch: 28 batch:  100 [ 25600/78200]  loss: 0.01804920\n",
      "epoch: 28 batch:  200 [ 51200/78200]  loss: 0.01809192\n",
      "epoch: 28 batch:  300 [ 76800/78200]  loss: 0.01802329\n",
      "epoch: 29 batch:  100 [ 25600/78200]  loss: 0.01739021\n",
      "epoch: 29 batch:  200 [ 51200/78200]  loss: 0.01835236\n",
      "epoch: 29 batch:  300 [ 76800/78200]  loss: 0.01807537\n",
      "epoch: 30 batch:  100 [ 25600/78200]  loss: 0.01739011\n",
      "epoch: 30 batch:  200 [ 51200/78200]  loss: 0.01743313\n",
      "epoch: 30 batch:  300 [ 76800/78200]  loss: 0.01750555\n",
      "epoch: 31 batch:  100 [ 25600/78200]  loss: 0.01752110\n",
      "epoch: 31 batch:  200 [ 51200/78200]  loss: 0.01750646\n",
      "epoch: 31 batch:  300 [ 76800/78200]  loss: 0.01824596\n",
      "epoch: 32 batch:  100 [ 25600/78200]  loss: 0.01847572\n",
      "epoch: 32 batch:  200 [ 51200/78200]  loss: 0.01737786\n",
      "epoch: 32 batch:  300 [ 76800/78200]  loss: 0.01791009\n",
      "epoch: 33 batch:  100 [ 25600/78200]  loss: 0.01748809\n",
      "epoch: 33 batch:  200 [ 51200/78200]  loss: 0.01827585\n",
      "epoch: 33 batch:  300 [ 76800/78200]  loss: 0.01705396\n",
      "epoch: 34 batch:  100 [ 25600/78200]  loss: 0.01773976\n",
      "epoch: 34 batch:  200 [ 51200/78200]  loss: 0.01723057\n",
      "epoch: 34 batch:  300 [ 76800/78200]  loss: 0.01703560\n",
      "epoch: 35 batch:  100 [ 25600/78200]  loss: 0.01765077\n",
      "epoch: 35 batch:  200 [ 51200/78200]  loss: 0.01799620\n",
      "epoch: 35 batch:  300 [ 76800/78200]  loss: 0.01687426\n",
      "epoch: 36 batch:  100 [ 25600/78200]  loss: 0.01760087\n",
      "epoch: 36 batch:  200 [ 51200/78200]  loss: 0.01719888\n",
      "epoch: 36 batch:  300 [ 76800/78200]  loss: 0.01771452\n",
      "epoch: 37 batch:  100 [ 25600/78200]  loss: 0.01733830\n",
      "epoch: 37 batch:  200 [ 51200/78200]  loss: 0.01762382\n",
      "epoch: 37 batch:  300 [ 76800/78200]  loss: 0.01739808\n",
      "epoch: 38 batch:  100 [ 25600/78200]  loss: 0.01748047\n",
      "epoch: 38 batch:  200 [ 51200/78200]  loss: 0.01717798\n",
      "epoch: 38 batch:  300 [ 76800/78200]  loss: 0.01696284\n",
      "epoch: 39 batch:  100 [ 25600/78200]  loss: 0.01778270\n",
      "epoch: 39 batch:  200 [ 51200/78200]  loss: 0.01767229\n",
      "epoch: 39 batch:  300 [ 76800/78200]  loss: 0.01718382\n",
      "epoch: 40 batch:  100 [ 25600/78200]  loss: 0.01702081\n",
      "epoch: 40 batch:  200 [ 51200/78200]  loss: 0.01724516\n",
      "epoch: 40 batch:  300 [ 76800/78200]  loss: 0.01754312\n",
      "epoch: 41 batch:  100 [ 25600/78200]  loss: 0.01804280\n",
      "epoch: 41 batch:  200 [ 51200/78200]  loss: 0.01791986\n",
      "epoch: 41 batch:  300 [ 76800/78200]  loss: 0.01697003\n",
      "epoch: 42 batch:  100 [ 25600/78200]  loss: 0.01680390\n",
      "epoch: 42 batch:  200 [ 51200/78200]  loss: 0.01719986\n",
      "epoch: 42 batch:  300 [ 76800/78200]  loss: 0.01645235\n",
      "epoch: 43 batch:  100 [ 25600/78200]  loss: 0.01653995\n",
      "epoch: 43 batch:  200 [ 51200/78200]  loss: 0.01703123\n",
      "epoch: 43 batch:  300 [ 76800/78200]  loss: 0.01713705\n",
      "epoch: 44 batch:  100 [ 25600/78200]  loss: 0.01811765\n",
      "epoch: 44 batch:  200 [ 51200/78200]  loss: 0.01804583\n",
      "epoch: 44 batch:  300 [ 76800/78200]  loss: 0.01704424\n",
      "epoch: 45 batch:  100 [ 25600/78200]  loss: 0.01704145\n",
      "epoch: 45 batch:  200 [ 51200/78200]  loss: 0.01706227\n",
      "epoch: 45 batch:  300 [ 76800/78200]  loss: 0.01705389\n",
      "epoch: 46 batch:  100 [ 25600/78200]  loss: 0.01665185\n",
      "epoch: 46 batch:  200 [ 51200/78200]  loss: 0.01789632\n",
      "epoch: 46 batch:  300 [ 76800/78200]  loss: 0.01692659\n",
      "epoch: 47 batch:  100 [ 25600/78200]  loss: 0.01647125\n",
      "epoch: 47 batch:  200 [ 51200/78200]  loss: 0.01725637\n",
      "epoch: 47 batch:  300 [ 76800/78200]  loss: 0.01695634\n",
      "epoch: 48 batch:  100 [ 25600/78200]  loss: 0.01710700\n",
      "epoch: 48 batch:  200 [ 51200/78200]  loss: 0.01710999\n",
      "epoch: 48 batch:  300 [ 76800/78200]  loss: 0.01737013\n",
      "epoch: 49 batch:  100 [ 25600/78200]  loss: 0.01715959\n",
      "epoch: 49 batch:  200 [ 51200/78200]  loss: 0.01679212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49 batch:  300 [ 76800/78200]  loss: 0.01621786\n",
      "epoch: 50 batch:  100 [ 25600/78200]  loss: 0.01666618\n",
      "epoch: 50 batch:  200 [ 51200/78200]  loss: 0.01626888\n",
      "epoch: 50 batch:  300 [ 76800/78200]  loss: 0.01598421\n",
      "epoch: 51 batch:  100 [ 25600/78200]  loss: 0.01707932\n",
      "epoch: 51 batch:  200 [ 51200/78200]  loss: 0.01675444\n",
      "epoch: 51 batch:  300 [ 76800/78200]  loss: 0.01676785\n",
      "epoch: 52 batch:  100 [ 25600/78200]  loss: 0.01757481\n",
      "epoch: 52 batch:  200 [ 51200/78200]  loss: 0.01620903\n",
      "epoch: 52 batch:  300 [ 76800/78200]  loss: 0.01660142\n",
      "epoch: 53 batch:  100 [ 25600/78200]  loss: 0.01720817\n",
      "epoch: 53 batch:  200 [ 51200/78200]  loss: 0.01657968\n",
      "epoch: 53 batch:  300 [ 76800/78200]  loss: 0.01634727\n",
      "epoch: 54 batch:  100 [ 25600/78200]  loss: 0.01675836\n",
      "epoch: 54 batch:  200 [ 51200/78200]  loss: 0.01606390\n",
      "epoch: 54 batch:  300 [ 76800/78200]  loss: 0.01763296\n",
      "epoch: 55 batch:  100 [ 25600/78200]  loss: 0.01711484\n",
      "epoch: 55 batch:  200 [ 51200/78200]  loss: 0.01663565\n",
      "epoch: 55 batch:  300 [ 76800/78200]  loss: 0.01686603\n",
      "epoch: 56 batch:  100 [ 25600/78200]  loss: 0.01679783\n",
      "epoch: 56 batch:  200 [ 51200/78200]  loss: 0.01671088\n",
      "epoch: 56 batch:  300 [ 76800/78200]  loss: 0.01677659\n",
      "epoch: 57 batch:  100 [ 25600/78200]  loss: 0.01698176\n",
      "epoch: 57 batch:  200 [ 51200/78200]  loss: 0.01743060\n",
      "epoch: 57 batch:  300 [ 76800/78200]  loss: 0.01596528\n",
      "epoch: 58 batch:  100 [ 25600/78200]  loss: 0.01668952\n",
      "epoch: 58 batch:  200 [ 51200/78200]  loss: 0.01703962\n",
      "epoch: 58 batch:  300 [ 76800/78200]  loss: 0.01648288\n",
      "epoch: 59 batch:  100 [ 25600/78200]  loss: 0.01664972\n",
      "epoch: 59 batch:  200 [ 51200/78200]  loss: 0.01676673\n",
      "epoch: 59 batch:  300 [ 76800/78200]  loss: 0.01655476\n",
      "epoch: 60 batch:  100 [ 25600/78200]  loss: 0.01611254\n",
      "epoch: 60 batch:  200 [ 51200/78200]  loss: 0.01734246\n",
      "epoch: 60 batch:  300 [ 76800/78200]  loss: 0.01673499\n",
      "epoch: 61 batch:  100 [ 25600/78200]  loss: 0.01635620\n",
      "epoch: 61 batch:  200 [ 51200/78200]  loss: 0.01645827\n",
      "epoch: 61 batch:  300 [ 76800/78200]  loss: 0.01681462\n",
      "epoch: 62 batch:  100 [ 25600/78200]  loss: 0.01624896\n",
      "epoch: 62 batch:  200 [ 51200/78200]  loss: 0.01729476\n",
      "epoch: 62 batch:  300 [ 76800/78200]  loss: 0.01626670\n",
      "epoch: 63 batch:  100 [ 25600/78200]  loss: 0.01622255\n",
      "epoch: 63 batch:  200 [ 51200/78200]  loss: 0.01652892\n",
      "epoch: 63 batch:  300 [ 76800/78200]  loss: 0.01657591\n",
      "epoch: 64 batch:  100 [ 25600/78200]  loss: 0.01618795\n",
      "epoch: 64 batch:  200 [ 51200/78200]  loss: 0.01662378\n",
      "epoch: 64 batch:  300 [ 76800/78200]  loss: 0.01684812\n",
      "epoch: 65 batch:  100 [ 25600/78200]  loss: 0.01659692\n",
      "epoch: 65 batch:  200 [ 51200/78200]  loss: 0.01700372\n",
      "epoch: 65 batch:  300 [ 76800/78200]  loss: 0.01668103\n",
      "epoch: 66 batch:  100 [ 25600/78200]  loss: 0.01687831\n",
      "epoch: 66 batch:  200 [ 51200/78200]  loss: 0.01596648\n",
      "epoch: 66 batch:  300 [ 76800/78200]  loss: 0.01575272\n",
      "epoch: 67 batch:  100 [ 25600/78200]  loss: 0.01622751\n",
      "epoch: 67 batch:  200 [ 51200/78200]  loss: 0.01683588\n",
      "epoch: 67 batch:  300 [ 76800/78200]  loss: 0.01656302\n",
      "epoch: 68 batch:  100 [ 25600/78200]  loss: 0.01637558\n",
      "epoch: 68 batch:  200 [ 51200/78200]  loss: 0.01620317\n",
      "epoch: 68 batch:  300 [ 76800/78200]  loss: 0.01571677\n",
      "epoch: 69 batch:  100 [ 25600/78200]  loss: 0.01663012\n",
      "epoch: 69 batch:  200 [ 51200/78200]  loss: 0.01642454\n",
      "epoch: 69 batch:  300 [ 76800/78200]  loss: 0.01672825\n",
      "epoch: 70 batch:  100 [ 25600/78200]  loss: 0.01630102\n",
      "epoch: 70 batch:  200 [ 51200/78200]  loss: 0.01648379\n",
      "epoch: 70 batch:  300 [ 76800/78200]  loss: 0.01686736\n",
      "epoch: 71 batch:  100 [ 25600/78200]  loss: 0.01645114\n",
      "epoch: 71 batch:  200 [ 51200/78200]  loss: 0.01612705\n",
      "epoch: 71 batch:  300 [ 76800/78200]  loss: 0.01689067\n",
      "epoch: 72 batch:  100 [ 25600/78200]  loss: 0.01616198\n",
      "epoch: 72 batch:  200 [ 51200/78200]  loss: 0.01633279\n",
      "epoch: 72 batch:  300 [ 76800/78200]  loss: 0.01651730\n",
      "epoch: 73 batch:  100 [ 25600/78200]  loss: 0.01602436\n",
      "epoch: 73 batch:  200 [ 51200/78200]  loss: 0.01693411\n",
      "epoch: 73 batch:  300 [ 76800/78200]  loss: 0.01676004\n",
      "epoch: 74 batch:  100 [ 25600/78200]  loss: 0.01663657\n",
      "epoch: 74 batch:  200 [ 51200/78200]  loss: 0.01578533\n",
      "epoch: 74 batch:  300 [ 76800/78200]  loss: 0.01666821\n",
      "epoch: 75 batch:  100 [ 25600/78200]  loss: 0.01586246\n",
      "epoch: 75 batch:  200 [ 51200/78200]  loss: 0.01605527\n",
      "epoch: 75 batch:  300 [ 76800/78200]  loss: 0.01686197\n",
      "epoch: 76 batch:  100 [ 25600/78200]  loss: 0.01650668\n",
      "epoch: 76 batch:  200 [ 51200/78200]  loss: 0.01591731\n",
      "epoch: 76 batch:  300 [ 76800/78200]  loss: 0.01594506\n",
      "epoch: 77 batch:  100 [ 25600/78200]  loss: 0.01604112\n",
      "epoch: 77 batch:  200 [ 51200/78200]  loss: 0.01607853\n",
      "epoch: 77 batch:  300 [ 76800/78200]  loss: 0.01660672\n",
      "epoch: 78 batch:  100 [ 25600/78200]  loss: 0.01633355\n",
      "epoch: 78 batch:  200 [ 51200/78200]  loss: 0.01637256\n",
      "epoch: 78 batch:  300 [ 76800/78200]  loss: 0.01678551\n",
      "epoch: 79 batch:  100 [ 25600/78200]  loss: 0.01541314\n",
      "epoch: 79 batch:  200 [ 51200/78200]  loss: 0.01660160\n",
      "epoch: 79 batch:  300 [ 76800/78200]  loss: 0.01668085\n",
      "epoch: 80 batch:  100 [ 25600/78200]  loss: 0.01634378\n",
      "epoch: 80 batch:  200 [ 51200/78200]  loss: 0.01618237\n",
      "epoch: 80 batch:  300 [ 76800/78200]  loss: 0.01685818\n",
      "epoch: 81 batch:  100 [ 25600/78200]  loss: 0.01654719\n",
      "epoch: 81 batch:  200 [ 51200/78200]  loss: 0.01677642\n",
      "epoch: 81 batch:  300 [ 76800/78200]  loss: 0.01700152\n",
      "epoch: 82 batch:  100 [ 25600/78200]  loss: 0.01642102\n",
      "epoch: 82 batch:  200 [ 51200/78200]  loss: 0.01696104\n",
      "epoch: 82 batch:  300 [ 76800/78200]  loss: 0.01655301\n",
      "epoch: 83 batch:  100 [ 25600/78200]  loss: 0.01607040\n",
      "epoch: 83 batch:  200 [ 51200/78200]  loss: 0.01628056\n",
      "epoch: 83 batch:  300 [ 76800/78200]  loss: 0.01650249\n",
      "epoch: 84 batch:  100 [ 25600/78200]  loss: 0.01612954\n",
      "epoch: 84 batch:  200 [ 51200/78200]  loss: 0.01574208\n",
      "epoch: 84 batch:  300 [ 76800/78200]  loss: 0.01630596\n",
      "epoch: 85 batch:  100 [ 25600/78200]  loss: 0.01673181\n",
      "epoch: 85 batch:  200 [ 51200/78200]  loss: 0.01690330\n",
      "epoch: 85 batch:  300 [ 76800/78200]  loss: 0.01641485\n",
      "epoch: 86 batch:  100 [ 25600/78200]  loss: 0.01641938\n",
      "epoch: 86 batch:  200 [ 51200/78200]  loss: 0.01579370\n",
      "epoch: 86 batch:  300 [ 76800/78200]  loss: 0.01583561\n",
      "epoch: 87 batch:  100 [ 25600/78200]  loss: 0.01623963\n",
      "epoch: 87 batch:  200 [ 51200/78200]  loss: 0.01724789\n",
      "epoch: 87 batch:  300 [ 76800/78200]  loss: 0.01625630\n",
      "epoch: 88 batch:  100 [ 25600/78200]  loss: 0.01555107\n",
      "epoch: 88 batch:  200 [ 51200/78200]  loss: 0.01706027\n",
      "epoch: 88 batch:  300 [ 76800/78200]  loss: 0.01594611\n",
      "epoch: 89 batch:  100 [ 25600/78200]  loss: 0.01617381\n",
      "epoch: 89 batch:  200 [ 51200/78200]  loss: 0.01535935\n",
      "epoch: 89 batch:  300 [ 76800/78200]  loss: 0.01568750\n",
      "epoch: 90 batch:  100 [ 25600/78200]  loss: 0.01567094\n",
      "epoch: 90 batch:  200 [ 51200/78200]  loss: 0.01634986\n",
      "epoch: 90 batch:  300 [ 76800/78200]  loss: 0.01568805\n",
      "epoch: 91 batch:  100 [ 25600/78200]  loss: 0.01720571\n",
      "epoch: 91 batch:  200 [ 51200/78200]  loss: 0.01576068\n",
      "epoch: 91 batch:  300 [ 76800/78200]  loss: 0.01663252\n",
      "epoch: 92 batch:  100 [ 25600/78200]  loss: 0.01619001\n",
      "epoch: 92 batch:  200 [ 51200/78200]  loss: 0.01601574\n",
      "epoch: 92 batch:  300 [ 76800/78200]  loss: 0.01631254\n",
      "epoch: 93 batch:  100 [ 25600/78200]  loss: 0.01637614\n",
      "epoch: 93 batch:  200 [ 51200/78200]  loss: 0.01709850\n",
      "epoch: 93 batch:  300 [ 76800/78200]  loss: 0.01643889\n",
      "epoch: 94 batch:  100 [ 25600/78200]  loss: 0.01684341\n",
      "epoch: 94 batch:  200 [ 51200/78200]  loss: 0.01646043\n",
      "epoch: 94 batch:  300 [ 76800/78200]  loss: 0.01606936\n",
      "epoch: 95 batch:  100 [ 25600/78200]  loss: 0.01652611\n",
      "epoch: 95 batch:  200 [ 51200/78200]  loss: 0.01613981\n",
      "epoch: 95 batch:  300 [ 76800/78200]  loss: 0.01655139\n",
      "epoch: 96 batch:  100 [ 25600/78200]  loss: 0.01561052\n",
      "epoch: 96 batch:  200 [ 51200/78200]  loss: 0.01609515\n",
      "epoch: 96 batch:  300 [ 76800/78200]  loss: 0.01584811\n",
      "epoch: 97 batch:  100 [ 25600/78200]  loss: 0.01586245\n",
      "epoch: 97 batch:  200 [ 51200/78200]  loss: 0.01589463\n",
      "epoch: 97 batch:  300 [ 76800/78200]  loss: 0.01657639\n",
      "epoch: 98 batch:  100 [ 25600/78200]  loss: 0.01608020\n",
      "epoch: 98 batch:  200 [ 51200/78200]  loss: 0.01617298\n",
      "epoch: 98 batch:  300 [ 76800/78200]  loss: 0.01595589\n",
      "epoch: 99 batch:  100 [ 25600/78200]  loss: 0.01552010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 99 batch:  200 [ 51200/78200]  loss: 0.01597271\n",
      "epoch: 99 batch:  300 [ 76800/78200]  loss: 0.01582485\n"
     ]
    }
   ],
   "source": [
    "lowest_loss = float(\"Inf\")\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for b, (X_train, y_train) in enumerate(train_loader):\n",
    "\n",
    "        b += 1\n",
    "\n",
    "        X_train = X_train.to(device)\n",
    "\n",
    "        # Apply the model\n",
    "        output = model(X_train)\n",
    "\n",
    "        loss = criterion(output, X_train)  # check loss with X_train itself\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if b % 100 == 0:\n",
    "            print(\n",
    "                f'epoch: {epoch:2} batch: {b:4} [{batch_size*b:6}/{train_data_len}]  '\n",
    "                + f'loss: {loss.item():.8f}')\n",
    "        if loss.item() < lowest_loss:\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "            lowest_loss = loss.item()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "        save_image(pic, './{}/image_{}.png'.format(decoded_out_dir, epoch))\n",
    "        #print(f\"saved model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = image_dim * image_dim\n",
    "\n",
    "sample_batches = 20\n",
    "decoded_data = torch.FloatTensor(batch_size,1,image_dim,image_dim)\n",
    "\n",
    "\n",
    "encoder_test = CNNEncoder(latent_dim=latent_dim)\n",
    "decoder_test = CNNDecoder(latent_dim=latent_dim)\n",
    "\n",
    "model_test = CNNAutoencoder(encoder_test, decoder_test,latent_dim=latent_dim)\n",
    "model_test.load_state_dict = torch.load(model_name)\n",
    "model_test.eval()\n",
    "\n",
    "if use_cuda:\n",
    "    model_test = model_test.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "for sb_ in range(sample_batches):\n",
    "    for i in range(batch_size):\n",
    "        z = torch.randn(1,latent_dim).to(device)\n",
    "        reconstructed_img = model_test.decoder(z).to('cpu')\n",
    "        img = reconstructed_img.view(image_dim, image_dim).data\n",
    "        decoded_data[i] = img\n",
    "\n",
    "    pic = to_img(decoded_data)\n",
    "    save_image(pic, './{}/image_decoded_{}.png'.format(decoded_out_dir,sb_))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 347.414818,
   "position": {
    "height": "469.025px",
    "left": "5.2px",
    "right": "20px",
    "top": "8.9875px",
    "width": "692.275px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
