{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n",
      "Using GPU: NVIDIA TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print('Running on device:', device)\n",
    "if use_cuda:\n",
    "    print('Using GPU:',\n",
    "          torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/therock/data2/devnagari_data/'\n",
    "\n",
    "expr_name = 'devnagari_cnn_vae'\n",
    "model_name = expr_name + '_PyTorch_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "# each image in dataset is 32x32 pixels\n",
    "image_dim = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize(image_dim),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "train_data = datasets.ImageFolder(os.path.join(root, 'Train'),\n",
    "                                  transform=train_transform)\n",
    "\n",
    "train_data_len = len(train_data)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_out_dir = expr_name + '_decoded'\n",
    "if not os.path.exists(decoded_out_dir):\n",
    "    os.mkdir(decoded_out_dir)\n",
    "\n",
    "\n",
    "def to_img(x):\n",
    "    x = x.view(x.size(0), 1, 32, 32)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_conv_out(n=1, p=1, f=1, s=1):\n",
    "        return int(((n + 2 * p - f) / s) + 1)\n",
    "\n",
    "def calc_deconv_out(n=1, p=1, f=1, s=1):\n",
    "        return int(s * (n - 1) + f - 2 * p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 30\n",
      "2: 15\n",
      "3: 11\n",
      "4: 5\n",
      "de 5: 7\n",
      "de 6: 15\n",
      "de 7: 31\n"
     ]
    }
   ],
   "source": [
    "conv1 = calc_conv_out(n=32, f=3, s=1, p=0)\n",
    "mp1 = calc_conv_out(n=conv1, f=2, s=2, p=0)\n",
    "conv2 = calc_conv_out(n=mp1, f=5, s=1, p=0)\n",
    "mp2 = calc_conv_out(n=conv2, f=2, s=2, p=0)\n",
    "deconv1 = calc_deconv_out(n=mp2, f=3, s=1, p=0)\n",
    "deconv2 = calc_deconv_out(n=deconv1, f=3, s=2, p=0)\n",
    "deconv3 = calc_deconv_out(n=deconv2, f=3, s=2, p=0)\n",
    "\n",
    "\n",
    "print('1:', conv1)\n",
    "print('2:', mp1)\n",
    "print('3:', conv2)\n",
    "print('4:', mp2)\n",
    "print('de 5:', deconv1)\n",
    "print('de 6:', deconv2)\n",
    "print('de 7:', deconv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim=64):\n",
    "\n",
    "        super(VAEEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=1, padding=0),  # b, 32, 30,30\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # b, 32, 15, 15\n",
    "            nn.Conv2d(16, 8, 5, stride=1, padding=0),  # b, 16, 11, 11\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2)  # b, 8, 5, 5\n",
    "        )\n",
    "        self.mu = nn.Linear(8*5*5, latent_dim)\n",
    "        self.var = nn.Linear(8*5*5, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.shape[0],8*5*5)\n",
    "        latent_mu = self.mu(x)\n",
    "        latent_var = self.var(x)\n",
    "        return latent_mu, latent_var\n",
    "\n",
    "class VAEDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim=10):\n",
    "\n",
    "        super(VAEDecoder, self).__init__()\n",
    "        self.l = nn.Linear(latent_dim, 8*5*5)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 4, 3, stride=1, padding=0),  # b, 12, 11,11\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(4, 2, 3, stride=2, padding=0),  # b, 6, 17, 17\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(2, 1, 3, stride=2, padding=0,\n",
    "                               output_padding=1),  # b, 1, 32, 32\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l(x)\n",
    "        x = x.view(x.shape[0],8,5,5)\n",
    "        x = self.decoder(x)\n",
    "        pred = torch.sigmoid(x)\n",
    "        return pred\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, encd, decd):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = encd\n",
    "        self.decoder = decd\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_mu, latent_var = self.encoder(x)\n",
    "\n",
    "        std = torch.exp(latent_var / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        x_sample = eps.mul(std).add_(latent_mu)\n",
    "\n",
    "        # decode\n",
    "        predicted = self.decoder(x_sample)\n",
    "        return predicted, latent_mu, latent_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (encoder): VAEEncoder(\n",
      "    (encoder): Sequential(\n",
      "      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): Conv2d(16, 8, kernel_size=(5, 5), stride=(1, 1))\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (mu): Linear(in_features=200, out_features=64, bias=True)\n",
      "    (var): Linear(in_features=200, out_features=64, bias=True)\n",
      "  )\n",
      "  (decoder): VAEDecoder(\n",
      "    (l): Linear(in_features=64, out_features=200, bias=True)\n",
      "    (decoder): Sequential(\n",
      "      (0): ConvTranspose2d(8, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): ConvTranspose2d(4, 2, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): ConvTranspose2d(2, 1, kernel_size=(3, 3), stride=(2, 2), output_padding=(1, 1))\n",
      "      (5): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "in_dim = image_dim * image_dim\n",
    "latent_dim = 64\n",
    "encoder = VAEEncoder(latent_dim=latent_dim)\n",
    "decoder = VAEDecoder(latent_dim=latent_dim)\n",
    "\n",
    "model = VAE(encoder, decoder)\n",
    "if use_cuda:\n",
    "    model = model.to(device)\n",
    "print(model)\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "lowest_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 : 144\n",
      " 1 : 16\n",
      " 2 : 3200\n",
      " 3 : 8\n",
      " 4 : 12800\n",
      " 5 : 64\n",
      " 6 : 12800\n",
      " 7 : 64\n",
      " 8 : 12800\n",
      " 9 : 200\n",
      "10 : 288\n",
      "11 : 4\n",
      "12 : 72\n",
      "13 : 2\n",
      "14 : 18\n",
      "15 : 1\n",
      "==========\n",
      " 42481\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for i, item in enumerate(params):\n",
    "        print(f'{i:2} : {item:}')\n",
    "    print(f'==========\\n{sum(params):>6}')\n",
    "\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train(image_dim, train_loader, model, e):\n",
    "    global lowest_loss\n",
    "    # set the train mode\n",
    "    model.train()\n",
    "\n",
    "    # loss of the epoch\n",
    "    train_loss = 0\n",
    "\n",
    "    for i, (x, _) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        x_sample, latent_mu, latent_var = model(x)\n",
    "\n",
    "        if e % 10 == 0:\n",
    "            pic = to_img(x_sample.cpu().data)\n",
    "            save_image(pic, './{}/image_{}.png'.format(decoded_out_dir, e))\n",
    "\n",
    "        # reconstruction loss\n",
    "        recon_loss = F.binary_cross_entropy(x_sample, x, reduction='sum')\n",
    "        # kl divergence loss\n",
    "        kl_loss = 0.5 * torch.sum(\n",
    "            torch.exp(latent_var) + latent_mu**2 - 1.0 - latent_var)\n",
    "\n",
    "        # total loss\n",
    "        loss = recon_loss + kl_loss\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        if loss.item() < lowest_loss:\n",
    "            lowest_loss = loss.item()\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "            #print(f\"saved model\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 629.9134830562659\n",
      "Epoch 1, Train Loss: 567.1098889066496\n",
      "Epoch 2, Train Loss: 557.3271369485294\n",
      "Epoch 3, Train Loss: 552.5030888347187\n",
      "Epoch 4, Train Loss: 548.8391469189578\n",
      "Epoch 5, Train Loss: 545.4971893981777\n",
      "Epoch 6, Train Loss: 542.7125213295236\n",
      "Epoch 7, Train Loss: 539.8243871882993\n",
      "Epoch 8, Train Loss: 535.8824558423913\n",
      "Epoch 9, Train Loss: 530.3134697190696\n",
      "Epoch 10, Train Loss: 526.1031888886669\n",
      "Epoch 11, Train Loss: 500.0064392183504\n",
      "Epoch 12, Train Loss: 489.4164639945652\n",
      "Epoch 13, Train Loss: 484.5542238451087\n",
      "Epoch 14, Train Loss: 481.6919627657449\n",
      "Epoch 15, Train Loss: 479.42204433743603\n",
      "Epoch 16, Train Loss: 477.01482731577687\n",
      "Epoch 17, Train Loss: 475.24334119245526\n",
      "Epoch 18, Train Loss: 473.7912414082481\n",
      "Epoch 19, Train Loss: 472.84519516264385\n",
      "Epoch 20, Train Loss: 472.0948720728101\n",
      "Epoch 21, Train Loss: 471.2701434622762\n",
      "Epoch 22, Train Loss: 470.48918762987535\n",
      "Epoch 23, Train Loss: 469.9128294337436\n",
      "Epoch 24, Train Loss: 469.3993243985774\n",
      "Epoch 25, Train Loss: 468.8342977241848\n",
      "Epoch 26, Train Loss: 468.2471711656809\n",
      "Epoch 27, Train Loss: 467.89324443534207\n",
      "Epoch 28, Train Loss: 467.6002881234015\n",
      "Epoch 29, Train Loss: 467.31706052189895\n",
      "Epoch 30, Train Loss: 467.1387584418958\n",
      "Epoch 31, Train Loss: 466.944736303149\n",
      "Epoch 32, Train Loss: 466.74160396019823\n",
      "Epoch 33, Train Loss: 466.59171240609015\n",
      "Epoch 34, Train Loss: 466.4569154112052\n",
      "Epoch 35, Train Loss: 466.3561299152813\n",
      "Epoch 36, Train Loss: 466.25482306985293\n",
      "Epoch 37, Train Loss: 466.1128565577046\n",
      "Epoch 38, Train Loss: 466.0467521679188\n",
      "Epoch 39, Train Loss: 465.8930144061701\n",
      "Epoch 40, Train Loss: 465.8782038742807\n",
      "Epoch 41, Train Loss: 465.765120434383\n",
      "Epoch 42, Train Loss: 465.68014176390665\n",
      "Epoch 43, Train Loss: 465.62469858935424\n",
      "Epoch 44, Train Loss: 465.5873336097346\n",
      "Epoch 45, Train Loss: 465.58629550631395\n",
      "Epoch 46, Train Loss: 465.4879706981298\n",
      "Epoch 47, Train Loss: 465.4047889526055\n",
      "Epoch 48, Train Loss: 465.3319448829124\n",
      "Epoch 49, Train Loss: 465.30704193973787\n",
      "Epoch 50, Train Loss: 465.27288263267263\n",
      "Epoch 51, Train Loss: 465.24635265145463\n",
      "Epoch 52, Train Loss: 465.1750062440058\n",
      "Epoch 53, Train Loss: 465.1713464574009\n",
      "Epoch 54, Train Loss: 465.12247017862853\n",
      "Epoch 55, Train Loss: 465.07564003556587\n",
      "Epoch 56, Train Loss: 465.00002607496805\n",
      "Epoch 57, Train Loss: 464.99813928628515\n",
      "Epoch 58, Train Loss: 464.97171585278136\n",
      "Epoch 59, Train Loss: 464.91895490329284\n",
      "Epoch 60, Train Loss: 464.8721493865889\n",
      "Epoch 61, Train Loss: 464.8155354359815\n",
      "Epoch 62, Train Loss: 464.8167054128037\n",
      "Epoch 63, Train Loss: 464.8015016084559\n",
      "Epoch 64, Train Loss: 464.7867922294597\n",
      "Epoch 65, Train Loss: 464.76987981537724\n",
      "Epoch 66, Train Loss: 464.73241093550195\n",
      "Epoch 67, Train Loss: 464.69921095748083\n",
      "Epoch 68, Train Loss: 464.6997052829284\n",
      "Epoch 69, Train Loss: 464.6610803128996\n",
      "Epoch 70, Train Loss: 464.58346087755757\n",
      "Epoch 71, Train Loss: 464.49461996483376\n",
      "Epoch 72, Train Loss: 464.57009261109334\n",
      "Epoch 73, Train Loss: 464.5295465852781\n",
      "Epoch 74, Train Loss: 464.50096172674233\n",
      "Epoch 75, Train Loss: 464.45641758911444\n",
      "Epoch 76, Train Loss: 464.4527118466272\n",
      "Epoch 77, Train Loss: 464.4477294796995\n",
      "Epoch 78, Train Loss: 464.36395370444376\n",
      "Epoch 79, Train Loss: 464.3733016304348\n",
      "Epoch 80, Train Loss: 464.3258923933024\n",
      "Epoch 81, Train Loss: 464.33166180466753\n",
      "Epoch 82, Train Loss: 464.3329608575767\n",
      "Epoch 83, Train Loss: 464.2655849384591\n",
      "Epoch 84, Train Loss: 464.26049767223464\n",
      "Epoch 85, Train Loss: 464.16960947490406\n",
      "Epoch 86, Train Loss: 464.197830982257\n",
      "Epoch 87, Train Loss: 464.24060252157926\n",
      "Epoch 88, Train Loss: 464.17405395820015\n",
      "Epoch 89, Train Loss: 464.16135095308505\n",
      "Epoch 90, Train Loss: 464.07813484055305\n",
      "Epoch 91, Train Loss: 464.11168303428707\n",
      "Epoch 92, Train Loss: 464.08503641504154\n",
      "Epoch 93, Train Loss: 464.0581455302909\n",
      "Epoch 94, Train Loss: 464.0413180346867\n",
      "Epoch 95, Train Loss: 464.05408218110614\n",
      "Epoch 96, Train Loss: 464.03345922914\n",
      "Epoch 97, Train Loss: 464.0167904811381\n",
      "Epoch 98, Train Loss: 464.00687190297316\n",
      "Epoch 99, Train Loss: 463.93244460318095\n"
     ]
    }
   ],
   "source": [
    "best_test_loss = float('inf')\n",
    "\n",
    "for e in range(num_epochs):\n",
    "\n",
    "    train_loss = train(image_dim, train_loader, model, e)\n",
    "    train_loss /= len(train_data)\n",
    "    print(f'Epoch {e}, Train Loss: {train_loss}')\n",
    "\n",
    "torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batches = 20\n",
    "decoded_data = torch.FloatTensor(batch_size,1,image_dim,image_dim)\n",
    "\n",
    "for sb_ in range(sample_batches):\n",
    "    for i in range(batch_size):\n",
    "        z = torch.randn(1, latent_dim).to(device)\n",
    "        reconstructed_img = model.decoder(z).to('cpu')\n",
    "        #print(f\"reconstructed_img {reconstructed_img.shape}\")\n",
    "        img = reconstructed_img.view(image_dim, image_dim).data\n",
    "\n",
    "    pic = to_img(decoded_data)\n",
    "    save_image(pic, './{}/image_decoded_{}.png'.format(decoded_out_dir,sb_))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 347.414818,
   "position": {
    "height": "469.025px",
    "left": "5.2px",
    "right": "20px",
    "top": "8.9875px",
    "width": "692.275px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
